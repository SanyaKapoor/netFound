{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed\n",
    "import numpy as np\n",
    "import utils\n",
    "import random\n",
    "from dataclasses import field, dataclass\n",
    "from datasets.distributed import split_dataset_by_node\n",
    "from typing import Optional\n",
    "from copy import deepcopy\n",
    "from torchinfo import summary\n",
    "from torch.distributed.elastic.multiprocessing.errors import record\n",
    "\n",
    "from transformers import (\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    top_k_accuracy_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from NetFoundDataCollator import DataCollatorForFlowClassification\n",
    "from NetFoundModels import NetfoundFinetuningModel, NetfoundNoPTM\n",
    "from NetFoundTrainer import NetfoundTrainer\n",
    "from NetfoundConfig import NetfoundConfig, NetFoundTCPOptionsConfig, NetFoundLarge\n",
    "from NetfoundTokenizer import NetFoundTokenizer\n",
    "\n",
    "from utils import ModelArguments, CommonDataTrainingArguments, freeze, verify_checkpoint, \\\n",
    "    load_train_test_datasets, get_90_percent_cpu_count, get_logger, init_tbwriter, update_deepspeed_config, \\\n",
    "    LearningRateLogCallback\n",
    "    \n",
    "random.seed(42)\n",
    "logger = get_logger(name=__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FineTuningDataTrainingArguments(CommonDataTrainingArguments):\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    num_labels: int = field(metadata={\"help\": \"number of classes in the datasets\"}, default=None)\n",
    "    problem_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Override regression or classification task\"},\n",
    "    )\n",
    "    p_val: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"noise rate\"\n",
    "        },\n",
    "    )\n",
    "    netfound_large: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Use the large configuration for netFound model\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(p: EvalPrediction):\n",
    "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    label_ids = p.label_ids.astype(int)\n",
    "    return {\"loss\": np.mean(np.absolute((logits - label_ids)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_metrics(p: EvalPrediction, num_classes):\n",
    "    logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    label_ids = p.label_ids.astype(int)\n",
    "    weighted_f1 = f1_score(\n",
    "        y_true=label_ids, y_pred=logits.argmax(axis=1), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    weighted_prec = precision_score(\n",
    "        y_true=label_ids, y_pred=logits.argmax(axis=1), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    weighted_recall = recall_score(\n",
    "        y_true=label_ids, y_pred=logits.argmax(axis=1), average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(y_true=label_ids, y_pred=logits.argmax(axis=1))\n",
    "    logger.warning(classification_report(label_ids, logits.argmax(axis=1), digits=5))\n",
    "    logger.warning(confusion_matrix(label_ids, logits.argmax(axis=1)))\n",
    "    if num_classes > 3:\n",
    "        logger.warning(f\"top3:{top_k_accuracy_score(label_ids, logits, k=3, labels=np.arange(num_classes))}\")\n",
    "    if num_classes > 5:\n",
    "        logger.warning(f\"top5:{top_k_accuracy_score(label_ids, logits, k=5, labels=np.arange(num_classes))}\")\n",
    "    if num_classes > 10:\n",
    "        logger.warning(f\"top10:{top_k_accuracy_score(label_ids, logits, k=10, labels=np.arange(num_classes))}\")\n",
    "    return {\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"weighted_prec: \": weighted_prec,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((ModelArguments, FineTuningDataTrainingArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=[\"--train_dir\", r\"D:\\AllStuff\\MTech-I_year\\SEMIV\\COL867\\Project\\netFound_original\\data\\test\\finetuning\\final\\combined\", \"--model_name_or_path\", r\"D:\\AllStuff\\MTech-I_year\\SEMIV\\COL867\\Project\\netFound_original\\models\\pretraining_original\", \"--output_dir\", r\"D:\\AllStuff\\MTech-I_year\\SEMIV\\COL867\\Project\\netFound_original\\models\\finetuning_original_6class\", \"--report_to\", \"tensorboard\", \"--overwrite_output_dir\", \"--save_safetensors\", \"false\", \"--do_train\", \"--do_eval\", \"--eval_strategy\", \"epoch\", \"--save_strategy\", \"epoch\", \"--learning_rate\", \"0.01\", \"--num_train_epochs\", \"1\", \"--problem_type\", \"single_label_classification\", \"--num_labels\", \"6\", \"--load_best_model_at_end\", \"--netfound_large\", \"True\"])\n",
    "# utils.LOGGING_LEVEL = training_args.get_process_log_level()\n",
    "utils.LOGGING_LEVEL = 10\n",
    "logger.setLevel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"model_args: {model_args}\")\n",
    "logger.info(f\"data_args: {data_args}\")\n",
    "logger.info(f\"training_args: {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_train_test_datasets(logger, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NetFoundTCPOptionsConfig if data_args.tcpoptions else NetfoundConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config(num_hidden_layers=model_args.num_hidden_layers, num_attention_heads=model_args.num_attention_heads,\n",
    "        hidden_size=model_args.hidden_size, no_meta=data_args.no_meta, flat=data_args.flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_args.netfound_large:\n",
    "    config.hidden_size = NetFoundLarge().hidden_size\n",
    "    config.num_hidden_layers = NetFoundLarge().num_hidden_layers\n",
    "    config.num_attention_heads = NetFoundLarge().num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.pretraining = False\n",
    "config.num_labels = data_args.num_labels\n",
    "config.problem_type = data_args.problem_type\n",
    "testingTokenizer = NetFoundTokenizer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = deepcopy(config)\n",
    "training_config.p = data_args.p_val\n",
    "training_config.limit_bursts = data_args.limit_bursts\n",
    "trainingTokenizer = NetFoundTokenizer(config=training_config)\n",
    "additionalFields = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"batched\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(function=trainingTokenizer, **params)\n",
    "test_dataset = test_dataset.map(function=testingTokenizer, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForFlowClassification(config.max_burst_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(f\"Using weights from {model_args.model_name_or_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = NetfoundFinetuningModel.from_pretrained(model_args.model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = freeze(original_model, model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = data_collator([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = model(labels = px['labels'], protocol = px['protocol'], flow_duration = px['flow_duration'], bytes = px['bytes'], iats = px['iats'], input_ids = px['input_ids'], attention_mask = px['attention_mask'], direction = px['direction'], pkt_count = px['pkt_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COL867",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
